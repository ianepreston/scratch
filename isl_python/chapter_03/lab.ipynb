{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.regressionplots import abline_plot\n",
    "import statsmodels.stats.outliers_influence as st_inf\n",
    "from sklearn.linear_model import LinearRegression\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.6.2 Simple Linear Regression\n",
    "\n",
    "Load the Boston housing dataset and perform a basic regression. The book uses R. I'm going to use statsmodels and scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sm.datasets.get_rdataset(\"Boston\", \"MASS\", cache=True).data\n",
    "df = sm.add_constant(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikit-learn implementation\n",
    "\n",
    "Not happy that I had to do this weird reshape just because I only had one independent variable. Maybe there's a better way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "boston_skl_ols = LinearRegression()\n",
    "boston_skl_ols.fit(df[\"lstat\"].to_numpy().reshape(-1, 1), df[\"medv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_skl_ols = LinearRegression()\n",
    "boston_skl_ols.fit(df[\"lstat\"].to_numpy().reshape(-1, 1), df[\"medv\"])\n",
    "print(\"Coefficients: \\n\", boston_skl_ols.coef_)\n",
    "print(\"Intercept: \\n\", boston_skl_ols.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it's not as verbose in estimation output, or at least I don't see an easy way to make it be, it's sure a lot faster, which makes sense given what it's designed for.\n",
    "\n",
    "I think I'll stick with StatsModels for the rest of this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StatsModels Implementation\n",
    "\n",
    "Where possible I prefer to use StatsModels. Scikit is great, and if I wanted to do a pure prediction I might prefer it, but StatsModels gives me all that analytic goodness I'm looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "boston_sm_ols = sm.OLS(df[\"medv\"], df[[\"const\", \"lstat\"]]).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run regression and display basic summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_sm_ols = sm.OLS(df[\"medv\"], df[[\"const\", \"lstat\"]]).fit()\n",
    "print(boston_sm_ols.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of the details are available in summary, but they can also be accessed using methods if you want to use them in further programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_sm_ols.conf_int(alpha=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate confidence and prediction intervals for a set of independent variables (note the call to add_constant to include the intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_sm_ols.get_prediction(sm.add_constant([5, 10, 15])).summary_frame(alpha=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot the regression there is a way to do it in StatsModels, but the nicer way is probably Seaborn, let's try both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x='lstat', y='medv', data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.plot(x='lstat', y='medv', kind='scatter')\n",
    "abline_plot(model_results=boston_sm_ols, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new dataframe for easier plotting\n",
    "result_df = df[[\"lstat\", \"medv\"]].copy()\n",
    "result_df[\"fitted\"] = boston_sm_ols.fittedvalues\n",
    "result_df[\"resid\"] = boston_sm_ols.resid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.plot(x=\"lstat\", y=\"resid\", kind=\"scatter\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot leverage against studentized residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "fig = sm.graphics.influence_plot(boston_sm_ols, ax=ax, criterion=\"cooks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can also get leverage and a bunch of other summary stats in numeric form, in ISL the lab shows which observation has the highest leverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_influence = st_inf.OLSInfluence(boston_sm_ols).summary_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_influence[\"hat_diag\"].idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ISL returns 375 for this, but I'm guessing that's just because R counts from 1 and python indices are from 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.6.3 Multiple Linear Regression\n",
    "\n",
    "Still using StatsModels, going to start using the same variable names as ISL going forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = sm.OLS(df[\"medv\"], df[[\"const\", \"lstat\", \"age\"]]).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=\"medv\")\n",
    "y = df[\"medv\"]\n",
    "lm = sm.OLS(y, X).fit()\n",
    "print(lm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More examples of accessing individual attributes. ISL pulls $R^2$ and RSE so I'll do the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"R squared: {lm.rsquared:0.3}, RSE: {lm.mse_resid:0.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate variance inflation factors for all the regressors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, col in enumerate(X.drop(columns=\"const\").columns):\n",
    "    var_inf = st_inf.variance_inflation_factor(X.to_numpy(), i + 1) # add one since we dropped constant\n",
    "    print(f\"VIF of {col}: {var_inf:0.2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interaction terms and non linear transformations\n",
    "\n",
    "I know statsmodels can use R style formulas to define these transformations, but I prefer to do it manually, maybe I'm just oldschool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"lstat_x_age\"] = X[\"lstat\"] * X[\"age\"]\n",
    "X[\"lstat_squared\"] = X[\"lstat\"]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sm.OLS(y, X[[\"const\", \"lstat\", \"age\", \"lstat_x_age\"]]).fit().summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sm.OLS(y, X[[\"const\", \"lstat\", \"lstat_squared\"]]).fit().summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANOVA to compare linear and quadratric models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_ols = sm.OLS(y, X[[\"const\", \"lstat\"]]).fit()\n",
    "quad_ols = sm.OLS(y, X[[\"const\", \"lstat\", \"lstat_squared\"]]).fit()\n",
    "sm.stats.anova_lm(linear_ols, quad_ols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = df[[\"lstat\", \"medv\"]].copy()\n",
    "result_df[\"quad_fitted\"] = quad_ols.fittedvalues\n",
    "result_df[\"quad_resid\"] = quad_ols.resid\n",
    "result_df[\"lin_fitted\"] = linear_ols.fittedvalues\n",
    "result_df[\"lin_resid\"] = linear_ols.resid\n",
    "cdf = result_df[[\"lstat\", \"quad_resid\", \"lin_resid\"]].melt(id_vars=\"lstat\", var_name=\"model\", value_name=\"resid\")\n",
    "sns.scatterplot(x=\"lstat\", y=\"resid\", hue=\"model\", data=cdf);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure we've improved the fit, but I don't see it as such a drastic improvement like the text claims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher order polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly = X[[\"const\", \"lstat\"]].copy()\n",
    "for i in range(2, 6):\n",
    "    new_name = \"lstat_pow_\" + str(i)\n",
    "    X_poly[new_name] = X_poly[\"lstat\"]**i\n",
    "poly_ols = sm.OLS(y, X_poly).fit()\n",
    "print(poly_ols.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that's way different than the book. The actual model fit is fine, but the coefficients are different. Further reading indicates that poly in R does an orthogonal polynomial. Quick googling didn't turn up any baked in way to compute that in python, but I found [this post](http://davmre.github.io/blog/python/2013/12/15/orthogonal_poly) that gives a method. Let's try that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ortho_poly_fit(x, degree = 1):\n",
    "    n = degree + 1\n",
    "    x = np.asarray(x).flatten()\n",
    "    if(degree >= len(np.unique(x))):\n",
    "            stop(\"'degree' must be less than number of unique points\")\n",
    "    xbar = np.mean(x)\n",
    "    x = x - xbar\n",
    "    X = np.fliplr(np.vander(x, n))\n",
    "    q,r = np.linalg.qr(X)\n",
    "\n",
    "    z = np.diag(np.diag(r))\n",
    "    raw = np.dot(q, z)\n",
    "\n",
    "    norm2 = np.sum(raw**2, axis=0)\n",
    "    alpha = (np.sum((raw**2)*np.reshape(x,(-1,1)), axis=0)/norm2 + xbar)[:degree]\n",
    "    Z = raw / np.sqrt(norm2)\n",
    "    return Z, norm2, alpha\n",
    "\n",
    "def ortho_poly_predict(x, alpha, norm2, degree = 1):\n",
    "    x = np.asarray(x).flatten()\n",
    "    n = degree + 1\n",
    "    Z = np.empty((len(x), n))\n",
    "    Z[:,0] = 1\n",
    "    if degree > 0:\n",
    "        Z[:, 1] = x - alpha[0]\n",
    "    if degree > 1:\n",
    "      for i in np.arange(1,degree):\n",
    "          Z[:, i+1] = (x - alpha[i]) * Z[:, i] - (norm2[i] / norm2[i-1]) * Z[:, i-1]\n",
    "    Z /= np.sqrt(norm2)\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstat = X[\"lstat\"]\n",
    "X_poly, _, _ = ortho_poly_fit(lstat, degree=5)\n",
    "poly_ols = sm.OLS(y, sm.add_constant(X_poly[:, 1:])).fit()\n",
    "print(poly_ols.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that fits now. The first column in the returned array is a weirdly scaled intercept, which is why I had to drop that column and add a constant back in... \n",
    "Not super elegant, but it does get the job done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
