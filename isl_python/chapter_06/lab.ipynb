{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we apply the best subset selection approach to the Hitters data. We wish to predict a baseball playerâ€™s Salary on the basis of various statistics associated with performance in the previous year. First of all, we note that the Salary variable is missing for some of the players. The is.na() function can be used to identify the missing observaitions. It returns a vector of the same length as the input vector, with a TRUE for any elements that are missing, and a FALSE for non-missing elements. The sum() function can then be used to count all of the missing elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sm.datasets.get_rdataset(\"Hitters\", \"ISLR\", cache=True).data.pipe(pd.get_dummies, columns=[\"League\", \"Division\", \"NewLeague\"], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Salary'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence we see that Salary is missing for 59 players. The na.omit() function removes all of the rows that have missing values in any variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"Salary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Salary'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regsubsets() function (part of the leaps library) performs best subset selection by identifying the best model that contains a given number of predictors, where best is quantified using RSS. The syntax is the same as for lm(). The summary() command outputs the best set of variables for each model size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fun times, doesn't look like python has an equivalent library so I guess I'm coding this by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"Salary\"]\n",
    "X = df.drop(columns=[\"Salary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's too slow to do all the way up to 8, let's just do it for 3. I'll get the point\n",
    "def modrsquared(coltuple):\n",
    "    lm = sm.OLS(y, sm.add_constant(X[[col for col in coltuple]])).fit()\n",
    "    return lm.rsquared\n",
    "\n",
    "models = dict()\n",
    "for i in range(1, 4):\n",
    "    col_opts = list(combinations(X.columns, i))\n",
    "    i_models = {cols: modrsquared(cols) for cols in col_opts}\n",
    "    best_cols = max(i_models.keys(), key=lambda k: i_models[k])\n",
    "    models[i] = best_cols\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary() function also returns $R^2$, RSS, adjusted $R^2$, $C_p$, and BIC. We can examine these to try to select the best overall model. For instance, we see that the $R^2$ statistic increases from 32%, when only one variable is included in the model, to almost 55 %, when all variables are included. As expected, the Plotting RSS, adjusted $R^2$, $C_p$, and BIC for all of the models at once will help us decide which model to select. Note the type=\"l\" option tells R to connect the plotted points with lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statsmodels has AIC but not C_p and since they're equivalent for OLS I'll just use AIC\n",
    "df = pd.DataFrame()\n",
    "for i in models.keys():\n",
    "    lm = sm.OLS(y, sm.add_constant(X[[col for col in models[i]]])).fit()\n",
    "    df.loc[i, \"R_square\"] = lm.rsquared\n",
    "    df.loc[i, \"adj_R_square\"] = lm.rsquared_adj\n",
    "    df.loc[i, \"RSS\"] = lm.mse_resid\n",
    "    df.loc[i, \"AIC\"] = lm.aic\n",
    "    df.loc[i, \"BIC\"] = lm.bic\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = df.reset_index().melt(id_vars=[\"index\"])\n",
    "sns.relplot(x=\"index\", y=\"value\", col=\"variable\", kind=\"line\", facet_kws={\"sharey\": False}, data=cdf);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5.2 Forward and Backward Stepwise Selection\n",
    "We can also use the ```regsubsets()``` function to perform forward stepwise or backward stepwise selection, using the argument ```method=\"forward\"``` or ```method=\"backward\"```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet, we don't have this in python either. \n",
    "I'll base my implementation on [this](https://planspace.org/20150423-forward_selection_with_statsmodels/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_selected(x, y, maxvars):\n",
    "    \"\"\"Linear model designed by forward selection.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    x: DataFrame, potential exogenous variables\n",
    "    y: Series, variable to predict\n",
    "    \"\"\"\n",
    "    remaining = set(x.columns)\n",
    "    selected = []\n",
    "    models = {}\n",
    "    current_score, best_new_score = 0.0, 0.0\n",
    "    while remaining and len(selected) <= maxvars and current_score == best_new_score:\n",
    "        scores_with_candidates = []\n",
    "        for candidate in remaining:\n",
    "            X_candidate = sm.add_constant(x[selected + [candidate]])\n",
    "            score = sm.OLS(y, X_candidate).fit().rsquared_adj\n",
    "            scores_with_candidates.append((score, candidate))\n",
    "        scores_with_candidates.sort()\n",
    "        best_new_score, best_candidate = scores_with_candidates.pop()\n",
    "        if current_score < best_new_score:\n",
    "            remaining.remove(best_candidate)\n",
    "            selected.append(best_candidate)\n",
    "            current_score = best_new_score\n",
    "            models[len(selected)] = selected[:]\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_models = forward_selected(X, y, maxvars=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_selected(x, y):\n",
    "    \"\"\"Linear model designed by forward selection.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    x: DataFrame, potential exogenous variables\n",
    "    y: Series, variable to predict\n",
    "    \"\"\"\n",
    "    selected = list(x.columns)\n",
    "    models = {}\n",
    "    while len(selected) > 1:\n",
    "        scores_with_candidates = []\n",
    "        for candidate in selected:\n",
    "            X_candidate = sm.add_constant(x[selected].drop(columns=[candidate]))\n",
    "            score = sm.OLS(y, X_candidate).fit().rsquared_adj\n",
    "            scores_with_candidates.append((score, candidate))\n",
    "        scores_with_candidates.sort()\n",
    "#         if len(scores_with_candidates) < 19:\n",
    "#             return scores_with_candidates\n",
    "        worst_score, worst_candidate = scores_with_candidates.pop(0)\n",
    "        selected.pop(selected.index(worst_candidate))\n",
    "        models[len(selected)] = selected[:]\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backward_models = backward_selected(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backward_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backward_models[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_models[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
