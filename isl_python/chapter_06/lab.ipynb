{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6 lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5.1 Best Subset Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we apply the best subset selection approach to the Hitters data. We wish to predict a baseball playerâ€™s Salary on the basis of various statistics associated with performance in the previous year. First of all, we note that the Salary variable is missing for some of the players. The is.na() function can be used to identify the missing observaitions. It returns a vector of the same length as the input vector, with a TRUE for any elements that are missing, and a FALSE for non-missing elements. The sum() function can then be used to count all of the missing elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sm.datasets.get_rdataset(\"Hitters\", \"ISLR\", cache=True).data.pipe(pd.get_dummies, columns=[\"League\", \"Division\", \"NewLeague\"], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Salary'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence we see that Salary is missing for 59 players. The na.omit() function removes all of the rows that have missing values in any variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"Salary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Salary'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regsubsets() function (part of the leaps library) performs best subset selection by identifying the best model that contains a given number of predictors, where best is quantified using RSS. The syntax is the same as for lm(). The summary() command outputs the best set of variables for each model size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fun times, doesn't look like python has an equivalent library so I guess I'm coding this by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"Salary\"]\n",
    "X = df.drop(columns=[\"Salary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's too slow to do all the way up to 8, let's just do it for 3. I'll get the point\n",
    "def modrsquared(coltuple):\n",
    "    lm = sm.OLS(y, sm.add_constant(X[[col for col in coltuple]])).fit()\n",
    "    return lm.rsquared\n",
    "\n",
    "models = dict()\n",
    "for i in range(1, 4):\n",
    "    col_opts = list(combinations(X.columns, i))\n",
    "    i_models = {cols: modrsquared(cols) for cols in col_opts}\n",
    "    best_cols = max(i_models.keys(), key=lambda k: i_models[k])\n",
    "    models[i] = best_cols\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary() function also returns $R^2$, RSS, adjusted $R^2$, $C_p$, and BIC. We can examine these to try to select the best overall model. For instance, we see that the $R^2$ statistic increases from 32%, when only one variable is included in the model, to almost 55 %, when all variables are included. As expected, the Plotting RSS, adjusted $R^2$, $C_p$, and BIC for all of the models at once will help us decide which model to select. Note the type=\"l\" option tells R to connect the plotted points with lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statsmodels has AIC but not C_p and since they're equivalent for OLS I'll just use AIC\n",
    "df = pd.DataFrame()\n",
    "for i in models.keys():\n",
    "    lm = sm.OLS(y, sm.add_constant(X[[col for col in models[i]]])).fit()\n",
    "    df.loc[i, \"R_square\"] = lm.rsquared\n",
    "    df.loc[i, \"adj_R_square\"] = lm.rsquared_adj\n",
    "    df.loc[i, \"RSS\"] = lm.mse_resid\n",
    "    df.loc[i, \"AIC\"] = lm.aic\n",
    "    df.loc[i, \"BIC\"] = lm.bic\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = df.reset_index().melt(id_vars=[\"index\"])\n",
    "sns.relplot(x=\"index\", y=\"value\", col=\"variable\", kind=\"line\", facet_kws={\"sharey\": False}, data=cdf);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5.2 Forward and Backward Stepwise Selection\n",
    "We can also use the ```regsubsets()``` function to perform forward stepwise or backward stepwise selection, using the argument ```method=\"forward\"``` or ```method=\"backward\"```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet, we don't have this in python either. \n",
    "I'll base my implementation on [this](https://planspace.org/20150423-forward_selection_with_statsmodels/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_selected(x, y, maxvars):\n",
    "    \"\"\"Linear model designed by forward selection.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    x: DataFrame, potential exogenous variables\n",
    "    y: Series, variable to predict\n",
    "    \"\"\"\n",
    "    remaining = set(x.columns)\n",
    "    selected = []\n",
    "    models = {}\n",
    "    current_score, best_new_score = 0.0, 0.0\n",
    "    while remaining and len(selected) <= maxvars and current_score == best_new_score:\n",
    "        scores_with_candidates = []\n",
    "        for candidate in remaining:\n",
    "            X_candidate = sm.add_constant(x[selected + [candidate]])\n",
    "            score = sm.OLS(y, X_candidate).fit().rsquared_adj\n",
    "            scores_with_candidates.append((score, candidate))\n",
    "        scores_with_candidates.sort()\n",
    "        best_new_score, best_candidate = scores_with_candidates.pop()\n",
    "        if current_score < best_new_score:\n",
    "            remaining.remove(best_candidate)\n",
    "            selected.append(best_candidate)\n",
    "            current_score = best_new_score\n",
    "            models[len(selected)] = selected[:]\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_models = forward_selected(X, y, maxvars=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_selected(x, y):\n",
    "    \"\"\"Linear model designed by forward selection.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    x: DataFrame, potential exogenous variables\n",
    "    y: Series, variable to predict\n",
    "    \"\"\"\n",
    "    selected = list(x.columns)\n",
    "    models = {}\n",
    "    while len(selected) > 1:\n",
    "        scores_with_candidates = []\n",
    "        for candidate in selected:\n",
    "            X_candidate = sm.add_constant(x[selected].drop(columns=[candidate]))\n",
    "            score = sm.OLS(y, X_candidate).fit().rsquared_adj\n",
    "            scores_with_candidates.append((score, X_candidate))\n",
    "        scores_with_candidates.sort()\n",
    "#         if len(scores_with_candidates) < 19:\n",
    "#             return scores_with_candidates\n",
    "        best_score, best_candidate = scores_with_candidates.pop()\n",
    "        selected = list(best_candidate.drop(columns=[\"const\"]).columns)\n",
    "        models[len(selected)] = selected[:]\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backward_models = backward_selected(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backward_models[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_models[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5.3 Choosing Among Models Using the Validation Set Approach and Cross-Validation\n",
    "\n",
    "We just saw that it is possible to choose among a set of models of different sizes using $C_p$, BIC, and adjusted $R^2$. We will now consider how to do this using the validation set and cross-validation approaches.\n",
    "\n",
    "In order for these approaches to yield accurate estimates of the test error, we must use *only the training observations* to perform all aspects of model-fitting - including variable selection. Therefore, the determination of which model of a given size is best must be made using *only the training observations*. This point is subtle but important. If the full data set is used to perform the best subset selection step, the validation set errors and cross-validation errors that we obtain will not be accurate estimates of the test error. \n",
    "\n",
    "In order to use the validation set approach, we begin by splitting the observations into a training set and a test set. \n",
    "\n",
    "Now we apply ```regsubsets()``` to the training set in order to perform best subset selection.\n",
    "\n",
    "Notice that we subset the ```Hitters``` data frame directly in the call in order to access only the training subset of the data, using the expression ```Hitters[train,]```. We now compute the validation set error for the best model of each model size. We first make a model matrix from the test data.\n",
    "\n",
    "The ```model.matrix()``` function is used in many regression packages for building an \"X\" matrix from data. Now we run a loop, and for each size ```i``` we extract the coefficients from ```regfit.best``` for the best model of that size, multiply them into the appropriate columns of the test model matrix to form the predictions, and compute the test MSE.\n",
    "\n",
    "We find that the best model is the one that contains ten variables.\n",
    "\n",
    "This was a little tedious, partly because there is no ```predict()``` method for ```regsubsets()```. Since we will be using this function again, we can capture our steps above and write our own predict method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
