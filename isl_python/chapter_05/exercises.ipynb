{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 applied exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split, LeaveOneOut, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5\n",
    "\n",
    "In chapter 4 we used logistic regression to predict the probability of *default* using *income* and *balance* on the *Default* data set. We will now estimate the test error of this logistic regression model using the validation set approach.\n",
    "\n",
    "a) Fit a logistic regression model that uses *income* and *balance* to predict *default*\n",
    "\n",
    "b) Using the validation set approach, estimate the test error of this mode.\n",
    "\n",
    "c) Repeat the process in b) 3 times, using 3 different splits of the observations into a training set and a test set. Comment on the results obtained.\n",
    "\n",
    "d) Now consider a logistic regression model that predicts the probability of *default* using *income*, *balance*, and a dummy variable for *student*. Estimate the test error for this model using the validation set approach. Comment on whether or not including a dummy variable for *student* leads to a reduction in the test error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ipreston/miniconda3/envs/isl/lib/python3.7/site-packages/statsmodels/datasets/utils.py:185: FutureWarning: `item` has been deprecated and will be removed in a future version\n",
      "  return dataset_meta[\"Title\"].item()\n"
     ]
    }
   ],
   "source": [
    "df = sm.datasets.get_rdataset(\"Default\", \"ISLR\", cache=True).data.pipe(pd.get_dummies, columns=[\"default\", \"student\"], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['balance', 'income', 'default_Yes', 'student_Yes'], dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ipreston/miniconda3/envs/isl/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "y = df[\"default_Yes\"]\n",
    "X = sm.add_constant(df[[\"income\", \"balance\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_validation_error():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    logit = sm.Logit(y_train, X_train).fit()\n",
    "    predict_prob = logit.predict(X_test)\n",
    "    predict_class = pd.Series(data=0, index=predict_prob.index)\n",
    "    predict_class.loc[predict_prob > 0.5] = 1\n",
    "    validation_error = (predict_class.values != y_test.values).mean()\n",
    "    return validation_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ipreston/miniconda3/envs/isl/lib/python3.7/site-packages/statsmodels/discrete/discrete_model.py:1736: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "/home/ipreston/miniconda3/envs/isl/lib/python3.7/site-packages/statsmodels/discrete/discrete_model.py:1789: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.sum(np.log(self.cdf(q*np.dot(X,params))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: inf\n",
      "         Iterations 10\n",
      "Optimization terminated successfully.\n",
      "         Current function value: inf\n",
      "         Iterations 10\n",
      "Optimization terminated successfully.\n",
      "         Current function value: inf\n",
      "         Iterations 10\n"
     ]
    }
   ],
   "source": [
    "validation_errors = [train_test_validation_error() for _ in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0244, 0.0244, 0.0224]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not really sure what to comment. They're all quite similar, which is good. If I did this a few more times I could make some distributional assumptions about the error rate. Note that this is a really imbalanced class, so my low error rate isn't that impressive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Now consider a logistic regression model that predicts the probability of *default* using *income*, *balance* and a dummy variable for *student*. Estimate the test error for this model using the validation set approach. Comment on whether or not including a dummy variable for *student* leads to a reduction in the test error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sm.add_constant(df[[\"income\", \"balance\", \"student_Yes\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: inf\n",
      "         Iterations 10\n",
      "Optimization terminated successfully.\n",
      "         Current function value: inf\n",
      "         Iterations 10\n",
      "Optimization terminated successfully.\n",
      "         Current function value: inf\n",
      "         Iterations 10\n"
     ]
    }
   ],
   "source": [
    "validation_errors = [train_test_validation_error() for _ in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.026, 0.0268, 0.0312]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inconclusive result from this small test. One result is lower than obtained without the dummy, one is higher, and one is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6\n",
    "\n",
    "We continue to consider the use of a logistic regression model to predict the probability of *default* using *income* and *balance* on the *Default* data set. In particular, we will now compute estimates for the standard errors of the *income* and *balance* logistic regression coefficients in two different ways: 1) using the bootstrap, and 2) using the standard formula for computing the standard errors. \n",
    "\n",
    "a) Use statsmodels to determine the estimated standard errors for the coefficients associated with *income* and *balance* in the multiple logistic regression model\n",
    "\n",
    "b) Write a function ```boot_fn```, that takes as input the *Default* data set as well as an index of observations,  and that outputs the coefficient estimates for *income* and *balance* in the multiple logistic regression model.\n",
    "\n",
    "c) Use ```boot_fn```  to estimate the standard errors of the logistic regression coefficients for *income* and *balance*\n",
    "\n",
    "d) comment on the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sm.datasets.get_rdataset(\"Default\", \"ISLR\", cache=True).data.pipe(pd.get_dummies, columns=[\"default\", \"student\"], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: inf\n",
      "         Iterations 10\n"
     ]
    }
   ],
   "source": [
    "y = df[\"default_Yes\"]\n",
    "X = sm.add_constant(df[[\"income\", \"balance\"]])\n",
    "logit = sm.Logit(y, X).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "const      0.434772\n",
       "income     0.000005\n",
       "balance    0.000227\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.bse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "const     -11.540468\n",
       "income      0.000021\n",
       "balance     0.005647\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boot_fn(base_df):\n",
    "    boot_df = base_df.sample(frac=1, replace=True)\n",
    "    y = boot_df[\"default_Yes\"]\n",
    "    X = sm.add_constant(boot_df[[\"income\", \"balance\"]])\n",
    "    logit = sm.Logit(y, X).fit(disp=0) # disp = 0 silences convergence notification\n",
    "    params = logit.params\n",
    "    return (params.loc[\"income\"], params.loc[\"balance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Income: parameter 2.101748831323398e-05, SE 4.782429969985769e-06\n",
      "Balance: parameter 0.005666488696159683, SE 0.0002278455134781629\n"
     ]
    }
   ],
   "source": [
    "income_params = list()\n",
    "balance_params = list()\n",
    "for _ in range(1_000):\n",
    "    income, balance = boot_fn(df)\n",
    "    income_params.append(income)\n",
    "    balance_params.append(balance)\n",
    "\n",
    "income_params = np.array(income_params)\n",
    "balance_params = np.array(balance_params)\n",
    "income_param_boot = np.mean(income_params)\n",
    "balance_param_boot = np.mean(balance_params)\n",
    "income_se_boot = np.std(income_params)\n",
    "balance_se_boot = np.std(balance_params)\n",
    "print(f\"Income: parameter {income_param_boot}, SE {income_se_boot}\")\n",
    "print(f\"Balance: parameter {balance_param_boot}, SE {balance_se_boot}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrap estimates of both the parameter values and standard error are quite close to the analytic solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7\n",
    "\n",
    "In sections 5.3.2 and 5.3.3, we saw that the ```cv.glm()``` function can be used in order to compute the LOOCV test error estimate. Alternatively, one could compute those quantities using just the ```glm()``` and ```predict.glm()``` functions, and a for loop. You will now take this approach in order to compute the LOOCV error for a simple logistic regression model on the ```Weekly``` data set. Recall that in the context of classification problems, the LOOCV error is given in 5.4:\n",
    "\n",
    "$CV_{(n)} = \\frac{1}{n} \\sum_{i=1}^{n} \\text{Err}_{i}$\n",
    "\n",
    "a) Fit a logistic regression model that predicts ```Direction``` using ```Lag1``` and ```Lag2```.\n",
    "\n",
    "b) Fit a logistic regression model that predicts ```Direction``` using ```Lag1``` and ```Lag2``` *using all but the first observation*.\n",
    "\n",
    "c) Use the model from (b) to predict the direction of the first observation. You can do this by predicting that the first observation will go up if P(```Direction=\"Up\"```|```Lag1,Lag2```) > 0.5. Was this observation correctly classified?\n",
    "\n",
    "d) Write a for loop from $i=1$ to $i=n$ where $n$ is the number of observations in the data set, that performs each of the following steps:\n",
    "* Fit a logistic regression model using all but the $i$th observation in order to predict whether or not the market moves up.\n",
    "* Compute the posterior probability for the $i$th observation in order to predict whether or not the market moves up.\n",
    "* Determine whether or not an error was made in predicting the direction for the $i$th observation. If an error was made, then indicate this with as a 1, and otherwise indicate it as a 0.\n",
    "\n",
    "e) Take the average of the $n$ numbers obtained by d) in order to obtain the LOOCV estimate of the test error. Comment on the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sm.datasets.get_rdataset(\"Weekly\", \"ISLR\", cache=True).data.pipe(pd.get_dummies, columns=[\"Direction\"], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"Direction_Up\"]\n",
    "X = sm.add_constant(df[[\"Lag1\", \"Lag2\"]])\n",
    "logit = sm.Logit(y, X).fit(disp=0) # disp = 0 silences convergence notification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_loo = sm.Logit(y.loc[1:], X.loc[1:]).fit(disp=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.57139232])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_loo.predict(X.loc[0].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation incorrectly classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5500459136822773"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = list()\n",
    "for index in y.index:\n",
    "    y_loo = y.loc[~y.index.isin([index])]\n",
    "    X_loo = X.loc[~X.index.isin([index])]\n",
    "    logit = sm.Logit(y_loo, X_loo).fit(disp=0)\n",
    "    pred = round(logit.predict(X.loc[index].values)[0], 0)\n",
    "    predictions.append(pred == y.loc[index])\n",
    "np.mean(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, right a little more than half the time, let's take this baby to the stock market!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8\n",
    "\n",
    "We will now perform cross-validation on a simulated data set.\n",
    "\n",
    "a) Generate a simulated data set as follows\n",
    "```\n",
    "> set.seed(1)\n",
    "> y=rnorm(100)\n",
    "> x= rnorm(100)\n",
    "> y = x - 2 * x^2 + rnorm(100)\n",
    "```\n",
    "\n",
    "In this data set, what is $n$ and what is $p$? Write out the model used to generate the data in equation form\n",
    "\n",
    "b) Create a scatterplot of $X$ against $Y$. Comment on what you find.\n",
    "\n",
    "c) Set a random seed, and then compute the LOOCV errors that result from fitting the following four models using least squares:\n",
    "* $ Y = \\beta_0 + \\beta_1 X + \\epsilon$\n",
    "* $ Y = \\beta_0 + \\beta_1 X + \\beta_2X^2 + \\epsilon$\n",
    "* $ Y = \\beta_0 + \\beta_1 X + \\beta_2X^2 + \\beta_2X^3 + \\epsilon$\n",
    "* $ Y = \\beta_0 + \\beta_1 X + \\beta_2X^2 + \\beta_2X^3 + \\beta_4X^4 + \\epsilon$\n",
    "\n",
    "Note that you may find it helpful to use the ```data.frame()``` function to create a single data set containing both $X$ and $Y$\n",
    "\n",
    "d) Repeat c) using another random seed, and report your results. Are they the same as c? Why?\n",
    "\n",
    "e) Which of the models in c) had the smallest LOOCV error? Is this what you expected? Explain your answer\n",
    "\n",
    "f) Comment on the statistical significance of the coefficient estimates that results from fitting each of the models in c) using least squares. Do these results agree with the conclusions drawn based on the cross validation results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_xy(seed=42, size=100):\n",
    "    np.random.seed(seed)\n",
    "    epsilon = np.random.normal(loc=0.0, scale=1.0, size=size)\n",
    "    x = np.random.normal(loc=0.0, scale=1.0, size=size)\n",
    "    y = x - (2 * x**2) + epsilon\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_poly(x, degrees):\n",
    "    poly = PolynomialFeatures(degrees, include_bias=False)\n",
    "    x_poly = poly.fit_transform(x.reshape(-1, 1))\n",
    "    return x_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loocv(seed, degrees, size=100):\n",
    "    x, y = gen_xy(seed, size)\n",
    "    x_poly = gen_poly(x, degrees)\n",
    "    ydf = pd.DataFrame(y, columns=[\"y\"])\n",
    "    xdf = pd.DataFrame(x_poly, index=ydf.index, columns=[f\"X_{i + 1}\" for i in range(x_poly.shape[1])])\n",
    "    errors = list()\n",
    "    for i in range(len(y)):\n",
    "        lm = LinearRegression()\n",
    "        x_loo = xdf.drop(i)\n",
    "        y_loo = ydf.drop(i)\n",
    "        lm.fit(x_loo, y_loo)\n",
    "        y_pred = lm.predict(xdf.loc[i].values.reshape(1, -1))[0][0]\n",
    "        errors.append(ydf.loc[i, \"y\"] - y_pred)\n",
    "    return np.mean(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial of degree 1 error: -0.0369\n",
      "Polynomial of degree 2 error: -0.0044\n",
      "Polynomial of degree 3 error: -0.0111\n",
      "Polynomial of degree 4 error: 0.0026\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 5, 1):\n",
    "    looc_err = loocv(42, i)\n",
    "    print(f\"Polynomial of degree {i} error: {looc_err:0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial of degree 1 error: -0.0370\n",
      "Polynomial of degree 2 error: -0.0020\n",
      "Polynomial of degree 3 error: -0.0077\n",
      "Polynomial of degree 4 error: -0.0050\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 5, 1):\n",
    "    looc_err = loocv(53, i)\n",
    "    print(f\"Polynomial of degree {i} error: {looc_err:0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They're different because I generated different data, I'm not sure if that was the intent of the question. If I hadn't then they'd obviously be the same. There's no randomness in the estimation component of LOOCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with the second degree polynomial has the lowest error, which is what I'd expect given that the true form of the model is a second degree polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def est_poly(degrees, seed=42, size=100):\n",
    "    x, y = gen_xy(seed, size)\n",
    "    x_poly = gen_poly(x, degrees)\n",
    "    ydf = pd.DataFrame(y, columns=[\"y\"])\n",
    "    xdf = pd.DataFrame(x_poly, index=ydf.index, columns=[f\"X_{i + 1}\" for i in range(x_poly.shape[1])])\n",
    "    lm = sm.OLS(ydf, sm.add_constant(xdf)).fit()\n",
    "    print(lm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.000\n",
      "Model:                            OLS   Adj. R-squared:                 -0.010\n",
      "Method:                 Least Squares   F-statistic:                   0.04064\n",
      "Date:                Sat, 11 Jan 2020   Prob (F-statistic):              0.841\n",
      "Time:                        18:11:10   Log-Likelihood:                -240.48\n",
      "No. Observations:                 100   AIC:                             485.0\n",
      "Df Residuals:                      98   BIC:                             490.2\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -1.8846      0.271     -6.959      0.000      -2.422      -1.347\n",
      "X_1            0.0575      0.285      0.202      0.841      -0.509       0.624\n",
      "==============================================================================\n",
      "Omnibus:                       52.505   Durbin-Watson:                   2.316\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              160.797\n",
      "Skew:                          -1.884   Prob(JB):                     1.21e-35\n",
      "Kurtosis:                       7.939   Cond. No.                         1.06\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.890\n",
      "Model:                            OLS   Adj. R-squared:                  0.887\n",
      "Method:                 Least Squares   F-statistic:                     391.5\n",
      "Date:                Sat, 11 Jan 2020   Prob (F-statistic):           3.57e-47\n",
      "Time:                        18:11:10   Log-Likelihood:                -130.25\n",
      "No. Observations:                 100   AIC:                             266.5\n",
      "Df Residuals:                      97   BIC:                             274.3\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.0309      0.112     -0.276      0.783      -0.253       0.192\n",
      "X_1            0.9020      0.100      9.028      0.000       0.704       1.100\n",
      "X_2           -2.0785      0.074    -27.974      0.000      -2.226      -1.931\n",
      "==============================================================================\n",
      "Omnibus:                        0.090   Durbin-Watson:                   2.085\n",
      "Prob(Omnibus):                  0.956   Jarque-Bera (JB):                0.058\n",
      "Skew:                          -0.052   Prob(JB):                        0.972\n",
      "Kurtosis:                       2.945   Cond. No.                         2.41\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.891\n",
      "Model:                            OLS   Adj. R-squared:                  0.887\n",
      "Method:                 Least Squares   F-statistic:                     260.7\n",
      "Date:                Sat, 11 Jan 2020   Prob (F-statistic):           5.38e-46\n",
      "Time:                        18:11:10   Log-Likelihood:                -129.83\n",
      "No. Observations:                 100   AIC:                             267.7\n",
      "Df Residuals:                      96   BIC:                             278.1\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.0697      0.120     -0.580      0.564      -0.309       0.169\n",
      "X_1            1.0304      0.175      5.890      0.000       0.683       1.378\n",
      "X_2           -2.0141      0.104    -19.452      0.000      -2.220      -1.809\n",
      "X_3           -0.0572      0.064     -0.894      0.373      -0.184       0.070\n",
      "==============================================================================\n",
      "Omnibus:                        0.057   Durbin-Watson:                   2.081\n",
      "Prob(Omnibus):                  0.972   Jarque-Bera (JB):                0.057\n",
      "Skew:                          -0.044   Prob(JB):                        0.972\n",
      "Kurtosis:                       2.922   Cond. No.                         7.61\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.896\n",
      "Model:                            OLS   Adj. R-squared:                  0.891\n",
      "Method:                 Least Squares   F-statistic:                     203.5\n",
      "Date:                Sat, 11 Jan 2020   Prob (F-statistic):           1.11e-45\n",
      "Time:                        18:11:10   Log-Likelihood:                -127.57\n",
      "No. Observations:                 100   AIC:                             265.1\n",
      "Df Residuals:                      95   BIC:                             278.2\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.1696      0.127     -1.330      0.187      -0.423       0.083\n",
      "X_1            0.8413      0.194      4.333      0.000       0.456       1.227\n",
      "X_2           -1.6895      0.185     -9.114      0.000      -2.057      -1.321\n",
      "X_3            0.0847      0.092      0.916      0.362      -0.099       0.268\n",
      "X_4           -0.0997      0.048     -2.095      0.039      -0.194      -0.005\n",
      "==============================================================================\n",
      "Omnibus:                        0.133   Durbin-Watson:                   2.083\n",
      "Prob(Omnibus):                  0.936   Jarque-Bera (JB):                0.219\n",
      "Skew:                          -0.083   Prob(JB):                        0.896\n",
      "Kurtosis:                       2.842   Cond. No.                         21.2\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ipreston/miniconda3/envs/isl/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "/home/ipreston/miniconda3/envs/isl/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "/home/ipreston/miniconda3/envs/isl/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "/home/ipreston/miniconda3/envs/isl/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 5, 1):\n",
    "    est_poly(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model that matches the functional form has all the right stats. $X$ and $X^2$ are significant, while the intercept is not. For the most part the other models follow that patter, but it's interesting that the model with just $X$ finds the intercept significant and not $X$. Similarly, the last model generally matches, except $X^4$ is significant when it's not actually a factor for $y$, suggesting overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9\n",
    "\n",
    "We will now consider the ```Boston``` housing data set, from the ```MASS``` library.\n",
    "\n",
    "a) Based on this data set, provide an estimate for the population mean of ```medv```. Call this estimate $\\hat{\\mu}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ipreston/miniconda3/envs/isl/lib/python3.7/site-packages/statsmodels/datasets/utils.py:185: FutureWarning: `item` has been deprecated and will be removed in a future version\n",
      "  return dataset_meta[\"Title\"].item()\n"
     ]
    }
   ],
   "source": [
    "df = sm.datasets.get_rdataset(\"Boston\", \"MASS\", cache=True).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>black</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      crim    zn  indus  chas    nox     rm   age     dis  rad  tax  ptratio  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
       "\n",
       "    black  lstat  medv  \n",
       "0  396.90   4.98  24.0  \n",
       "1  396.90   9.14  21.6  \n",
       "2  392.83   4.03  34.7  \n",
       "3  394.63   2.94  33.4  \n",
       "4  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.532806324110677"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu_hat = df[\"medv\"].mean()\n",
    "mu_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Provide an estimate of the standard error of $\\hat{\\mu}$. Interpret this result. \n",
    "\n",
    "*Hint: We can compute the standard error of the sample mean by dividing the sample standard deviation by the square root of the number of observations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40886114749753505"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu_se_hat = df[\"medv\"].std() / np.sqrt(len(df))\n",
    "mu_se_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a measure of the dispersion of the sample mean around the population mean. That is, it gives an estimate of how far off from the population mean this sample mean is likely to be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Now estimate the standard error of $\\hat{\\mu}$ using the bootstrap. How does this compare to your answer from (b)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_hats = [df[\"medv\"].sample(frac=1, replace=True).mean() for _ in range (1_000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.53752035573123"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(mu_hats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39635933590085537"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu_boot_se_hat = np.std(mu_hats)\n",
    "mu_boot_se_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Based on your bootstrap estimate from c), provide a 95% confidence interval for the mean of ```medv```. Compare it to the restuls obtained using ```t.test(Boston$medv)```.\n",
    "\n",
    "*Hint: You can approximate a 95% confidence interval using the formula* $[\\hat{\\mu} - 2\\text{SE}(\\hat{\\mu}), \\hat{\\mu} + 2\\text{SE}(\\hat{\\mu})]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21.740087652308965, 23.32552499591239)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boot_ci = (mu_hat - (2 * mu_boot_se_hat), mu_hat + (2 * mu_boot_se_hat))\n",
    "boot_ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21.73145320033779, 23.334159447883565)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_int = stats.norm.interval(0.95, loc=mu_hat, scale=mu_se_hat)\n",
    "conf_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, samesies!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Based on this data set, provide an estimate, $\\hat{\\mu}_{\\text{med}}$, for the median value of ```medv``` in the population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.2"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "med_hat = df[\"medv\"].median()\n",
    "med_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) We now would like to estimate the standard error of $\\hat{\\mu}_{\\text{med}}$. Unfortunately, there is no simple formula for computing the standard error of the median. Instead, estimate the standard error of the median using the bootstrap. Comment on your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37415429972138464"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "med_hats = [df[\"medv\"].sample(frac=1, replace=True).median() for _ in range(1_000)]\n",
    "med_boot_se_hat = np.std(med_hats)\n",
    "med_boot_se_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard error of the median is pretty close to the standard error of the mean. Suggests a relatively symmetrical distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) Based on this data set, provide an estimate for the tenth percentile of ```medv``` in Boston suburbs. Call the quantity $\\hat{\\mu}_{0.1}$ (You can use the ```quantile()``` function.)\n",
    "h) Use the bootstrap method to estimate the standard error of $\\hat{\\mu}_{0.1}$. Comment on your findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.75\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5004421819751009"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df[\"medv\"].quantile(.1))\n",
    "quant_hats = [df[\"medv\"].sample(frac=1, replace=True).quantile(.1) for _ in range(1_000)]\n",
    "quant_boot_se_hat = np.std(quant_hats)\n",
    "quant_boot_se_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard error is higher for the 10th quantile than the mean or median, suggesting greater uncertainty at the tail of the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
