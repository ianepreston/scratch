{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 applied exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, LeaveOneOut, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5\n",
    "\n",
    "In chapter 4 we used logistic regression to predict the probability of *default* using *income* and *balance* on the *Default* data set. We will now estimate the test error of this logistic regression model using the validation set approach.\n",
    "\n",
    "a) Fit a logistic regression model that uses *income* and *balance* to predict *default*\n",
    "\n",
    "b) Using the validation set approach, estimate the test error of this mode.\n",
    "\n",
    "c) Repeat the process in b) 3 times, using 3 different splits of the observations into a training set and a test set. Comment on the results obtained.\n",
    "\n",
    "d) Now consider a logistic regression model that predicts the probability of *default* using *income*, *balance*, and a dummy variable for *student*. Estimate the test error for this model using the validation set approach. Comment on whether or not including a dummy variable for *student* leads to a reduction in the test error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sm.datasets.get_rdataset(\"Default\", \"ISLR\", cache=True).data.pipe(pd.get_dummies, columns=[\"default\", \"student\"], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"default_Yes\"]\n",
    "X = sm.add_constant(df[[\"income\", \"balance\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_validation_error():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    logit = sm.Logit(y_train, X_train).fit()\n",
    "    predict_prob = logit.predict(X_test)\n",
    "    predict_class = pd.Series(data=0, index=predict_prob.index)\n",
    "    predict_class.loc[predict_prob > 0.5] = 1\n",
    "    validation_error = (predict_class.values != y_test.values).mean()\n",
    "    return validation_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_errors = [train_test_validation_error() for _ in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not really sure what to comment. They're all quite similar, which is good. If I did this a few more times I could make some distributional assumptions about the error rate. Note that this is a really imbalanced class, so my low error rate isn't that impressive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Now consider a logistic regression model that predicts the probability of *default* using *income*, *balance* and a dummy variable for *student*. Estimate the test error for this model using the validation set approach. Comment on whether or not including a dummy variable for *student* leads to a reduction in the test error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sm.add_constant(df[[\"income\", \"balance\", \"student_Yes\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_errors = [train_test_validation_error() for _ in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inconclusive result from this small test. One result is lower than obtained without the dummy, one is higher, and one is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6\n",
    "\n",
    "We continue to consider the use of a logistic regression model to predict the probability of *default* using *income* and *balance* on the *Default* data set. In particular, we will now compute estimates for the standard errors of the *income* and *balance* logistic regression coefficients in two different ways: 1) using the bootstrap, and 2) using the standard formula for computing the standard errors. \n",
    "\n",
    "a) Use statsmodels to determine the estimated standard errors for the coefficients associated with *income* and *balance* in the multiple logistic regression model\n",
    "\n",
    "b) Write a function ```boot_fn```, that takes as input the *Default* data set as well as an index of observations,  and that outputs the coefficient estimates for *income* and *balance* in the multiple logistic regression model.\n",
    "\n",
    "c) Use ```boot_fn```  to estimate the standard errors of the logistic regression coefficients for *income* and *balance*\n",
    "\n",
    "d) comment on the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sm.datasets.get_rdataset(\"Default\", \"ISLR\", cache=True).data.pipe(pd.get_dummies, columns=[\"default\", \"student\"], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"default_Yes\"]\n",
    "X = sm.add_constant(df[[\"income\", \"balance\"]])\n",
    "logit = sm.Logit(y, X).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit.bse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boot_fn(base_df):\n",
    "    boot_df = base_df.sample(frac=1, replace=True)\n",
    "    y = boot_df[\"default_Yes\"]\n",
    "    X = sm.add_constant(boot_df[[\"income\", \"balance\"]])\n",
    "    logit = sm.Logit(y, X).fit(disp=0) # disp = 0 silences convergence notification\n",
    "    params = logit.params\n",
    "    return (params.loc[\"income\"], params.loc[\"balance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_params = list()\n",
    "balance_params = list()\n",
    "for _ in range(1_000):\n",
    "    income, balance = boot_fn(df)\n",
    "    income_params.append(income)\n",
    "    balance_params.append(balance)\n",
    "\n",
    "income_params = np.array(income_params)\n",
    "balance_params = np.array(balance_params)\n",
    "income_param_boot = np.mean(income_params)\n",
    "balance_param_boot = np.mean(balance_params)\n",
    "income_se_boot = np.std(income_params)\n",
    "balance_se_boot = np.std(balance_params)\n",
    "print(f\"Income: parameter {income_param_boot}, SE {income_se_boot}\")\n",
    "print(f\"Balance: parameter {balance_param_boot}, SE {balance_se_boot}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrap estimates of both the parameter values and standard error are quite close to the analytic solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7\n",
    "\n",
    "In sections 5.3.2 and 5.3.3, we saw that the ```cv.glm()``` function can be used in order to compute the LOOCV test error estimate. Alternatively, one could compute those quantities using just the ```glm()``` and ```predict.glm()``` functions, and a for loop. You will now take this approach in order to compute the LOOCV error for a simple logistic regression model on the ```Weekly``` data set. Recall that in the context of classification problems, the LOOCV error is given in 5.4:\n",
    "\n",
    "$CV_{(n)} = \\frac{1}{n} \\sum_{i=1}^{n} \\text{Err}_{i}$\n",
    "\n",
    "a) Fit a logistic regression model that predicts ```Direction``` using ```Lag1``` and ```Lag2```.\n",
    "\n",
    "b) Fit a logistic regression model that predicts ```Direction``` using ```Lag1``` and ```Lag2``` *using all but the first observation*.\n",
    "\n",
    "c) Use the model from (b) to predict the direction of the first observation. You can do this by predicting that the first observation will go up if P(```Direction=\"Up\"```|```Lag1,Lag2```) > 0.5. Was this observation correctly classified?\n",
    "\n",
    "d) Write a for loop from $i=1$ to $i=n$ where $n$ is the number of observations in the data set, that performs each of the following steps:\n",
    "* Fit a logistic regression model using all but the $i$th observation in order to predict whether or not the market moves up.\n",
    "* Compute the posterior probability for the $i$th observation in order to predict whether or not the market moves up.\n",
    "* Determine whether or not an error was made in predicting the direction for the $i$th observation. If an error was made, then indicate this with as a 1, and otherwise indicate it as a 0.\n",
    "\n",
    "e) Take the average of the $n$ numbers obtained by d) in order to obtain the LOOCV estimate of the test error. Comment on the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sm.datasets.get_rdataset(\"Weekly\", \"ISLR\", cache=True).data.pipe(pd.get_dummies, columns=[\"Direction\"], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"Direction_Up\"]\n",
    "X = sm.add_constant(df[[\"Lag1\", \"Lag2\"]])\n",
    "logit = sm.Logit(y, X).fit(disp=0) # disp = 0 silences convergence notification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_loo = sm.Logit(y.loc[1:], X.loc[1:]).fit(disp=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_loo.predict(X.loc[0].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation incorrectly classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = list()\n",
    "for index in y.index:\n",
    "    y_loo = y.loc[~y.index.isin([index])]\n",
    "    X_loo = X.loc[~X.index.isin([index])]\n",
    "    logit = sm.Logit(y_loo, X_loo).fit(disp=0)\n",
    "    pred = round(logit.predict(X.loc[index].values)[0], 0)\n",
    "    predictions.append(pred == y.loc[index])\n",
    "np.mean(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, right a little more than half the time, let's take this baby to the stock market!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
