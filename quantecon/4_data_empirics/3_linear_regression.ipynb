{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "In the lecture, we think the original model suffers from endogeneity bias due to the likely effect income has on institutional development\n",
    "\n",
    "Although endogeneity is often best identified by thinking about the data and model, we can formally test for endogeneity using the **Hausman test**\n",
    "\n",
    "We want to test for correlation between the endogenous variable, $avexpr_i$ and the errors, $u_i$\n",
    "\n",
    "$H_0: Cov(avexpr_i, u_i) = 0 \\quad (\\text{no endogeneity})$\n",
    "\n",
    "$H_1: Cov(avexpr_i, u_i) \\neq 0 \\quad (\\text{endogeneity})$\n",
    "\n",
    "This test is run in two stages\n",
    "\n",
    "First, we regress $avexpr_i$ on the instrument $logem4_i$\n",
    "\n",
    "$avexpr_i = \\pi_0 + \\pi_1logem4_i + v_i$\n",
    "\n",
    "Second, we retrive the residuals $\\hat{v}_i$ and include them in the original equation\n",
    "\n",
    "$logpgp95_i = \\beta_0 + \\beta_1 avexpr_i + \\alpha \\hat{v}_i + u_i$\n",
    "\n",
    "If $\\alpha$ is statistically significant(with a p-value < 0.05), then we reject the null hypothesis and conclude $avexpr_i$ is endogenous\n",
    "\n",
    "Using the above information, estimate a Hausman test and interpret your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avexpr</th>\n",
       "      <th>logem4</th>\n",
       "      <th>logpgp95</th>\n",
       "      <th>constant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.363636</td>\n",
       "      <td>5.634789</td>\n",
       "      <td>7.770645</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.386364</td>\n",
       "      <td>4.232656</td>\n",
       "      <td>9.133459</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.318182</td>\n",
       "      <td>2.145931</td>\n",
       "      <td>9.897972</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.454545</td>\n",
       "      <td>5.634789</td>\n",
       "      <td>6.845880</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.136364</td>\n",
       "      <td>4.268438</td>\n",
       "      <td>6.877296</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     avexpr    logem4  logpgp95  constant\n",
       "0  5.363636  5.634789  7.770645         1\n",
       "1  6.386364  4.232656  9.133459         1\n",
       "2  9.318182  2.145931  9.897972         1\n",
       "3  4.454545  5.634789  6.845880         1\n",
       "4  5.136364  4.268438  6.877296         1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = (\n",
    "    pd.read_stata('https://github.com/QuantEcon/QuantEcon.lectures.code/raw/master/ols/maketable1.dta')\n",
    "    [['avexpr', 'logem4', 'logpgp95']]\n",
    "    .assign(constant=1)\n",
    "    .dropna()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 avexpr   R-squared:                       0.305\n",
      "Model:                            OLS   Adj. R-squared:                  0.294\n",
      "Method:                 Least Squares   F-statistic:                     29.80\n",
      "Date:                Wed, 27 Mar 2019   Prob (F-statistic):           7.29e-07\n",
      "Time:                        17:15:00   Log-Likelihood:                -116.90\n",
      "No. Observations:                  70   AIC:                             237.8\n",
      "Df Residuals:                      68   BIC:                             242.3\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "constant       9.5146      0.548     17.359      0.000       8.421      10.608\n",
      "logem4        -0.6314      0.116     -5.459      0.000      -0.862      -0.401\n",
      "==============================================================================\n",
      "Omnibus:                        0.286   Durbin-Watson:                   1.851\n",
      "Prob(Omnibus):                  0.867   Jarque-Bera (JB):                0.464\n",
      "Skew:                          -0.096   Prob(JB):                        0.793\n",
      "Kurtosis:                       2.650   Cond. No.                         17.3\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "get_resid_model = sm.OLS(\n",
    "    df['avexpr'],\n",
    "    df[['constant', 'logem4']],\n",
    ").fit()\n",
    "print(get_resid_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vhat'] = get_resid_model.resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:               logpgp95   R-squared:                       0.689\n",
      "Model:                            OLS   Adj. R-squared:                  0.679\n",
      "Method:                 Least Squares   F-statistic:                     74.05\n",
      "Date:                Wed, 27 Mar 2019   Prob (F-statistic):           1.07e-17\n",
      "Time:                        17:15:00   Log-Likelihood:                -62.031\n",
      "No. Observations:                  70   AIC:                             130.1\n",
      "Df Residuals:                      67   BIC:                             136.8\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "constant       2.3702      0.565      4.197      0.000       1.243       3.497\n",
      "avexpr         0.8684      0.084     10.304      0.000       0.700       1.037\n",
      "vhat          -0.5071      0.101     -5.017      0.000      -0.709      -0.305\n",
      "==============================================================================\n",
      "Omnibus:                       17.597   Durbin-Watson:                   2.086\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               23.194\n",
      "Skew:                          -1.054   Prob(JB):                     9.19e-06\n",
      "Kurtosis:                       4.873   Cond. No.                         55.5\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "check_endog = sm.OLS(\n",
    "    df['logpgp95'],\n",
    "    df[['constant', 'avexpr', 'vhat']]\n",
    ").fit()\n",
    "print(check_endog.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient on $\\hat{v}$ is statistically significant so we reject the null hypothesis and conclude there is endogeneity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "The OLS parameter $\\beta$ can also be estimated using matrix algebra and `numpy` (you may need to review the [numpy](https://lectures.quantecon.org/py/numpy.html) lecture to complete this exercise)\n",
    "\n",
    "The linear equation we want to estimate is (written in matrix form)\n",
    "\n",
    "$$y = X\\beta + u$$\n",
    "\n",
    "To solve for the unknown parameter $\\beta$ we want to minimise the sum of squared residuals\n",
    "\n",
    "$$\\underset{\\hat{\\beta}}{\\min} \\hat{u}'\\hat{u}$$\n",
    "\n",
    "Rearranging the first equation and substituting into the second equation, we can write\n",
    "\n",
    "$$\\underset{\\hat{\\beta}}{\\min} (Y - X\\hat{\\beta})' (Y - X\\hat{\\beta})$$\n",
    "\n",
    "Solving this optimization problem gives the solution for the $\\hat{\\beta}$ coefficients\n",
    "\n",
    "$$\\hat{\\beta} = (X'X)^{-1}X'y$$\n",
    "\n",
    "Using the above information, computer $\\hat{\\beta}$ from model 1 using `numpy` - your results should be the same as those in the `statsmodels` output from earlier in the lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    pd.read_stata('https://github.com/QuantEcon/QuantEcon.lectures.code/raw/master/ols/maketable1.dta')\n",
    "    [['avexpr', 'logpgp95']]\n",
    "    .assign(constant=1)\n",
    "    .dropna()\n",
    "    .reset_index(drop=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:               logpgp95   R-squared:                       0.611\n",
      "Model:                            OLS   Adj. R-squared:                  0.608\n",
      "Method:                 Least Squares   F-statistic:                     171.4\n",
      "Date:                Wed, 27 Mar 2019   Prob (F-statistic):           4.16e-24\n",
      "Time:                        17:15:01   Log-Likelihood:                -119.71\n",
      "No. Observations:                 111   AIC:                             243.4\n",
      "Df Residuals:                     109   BIC:                             248.8\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "constant       4.6261      0.301     15.391      0.000       4.030       5.222\n",
      "avexpr         0.5319      0.041     13.093      0.000       0.451       0.612\n",
      "==============================================================================\n",
      "Omnibus:                        9.251   Durbin-Watson:                   1.689\n",
      "Prob(Omnibus):                  0.010   Jarque-Bera (JB):                9.170\n",
      "Skew:                          -0.680   Prob(JB):                       0.0102\n",
      "Kurtosis:                       3.362   Cond. No.                         33.2\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Do Statsmodels way first for easy comparison\n",
    "ex2_mod = sm.OLS(\n",
    "    df['logpgp95'],\n",
    "    df[['constant', 'avexpr']]\n",
    ").fit()\n",
    "print(ex2_mod.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[['constant', 'avexpr']].to_numpy()\n",
    "y = df['logpgp95'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.62608941, 0.53187135])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_hat = np.linalg.inv(x.T @ x) @ (x.T @ y)\n",
    "b_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.62608941, 0.53187135])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 2 preferred solution\n",
    "b_hat2 = np.linalg.solve(x.T @ x, x.T @ y)\n",
    "b_hat2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
